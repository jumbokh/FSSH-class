{
  "cells": [
    {
      "metadata": {
        "_uuid": "bda30bb60722647ec9219ff4644dbf09991582bb",
        "id": "4GVBBSFBKoza"
      },
      "cell_type": "markdown",
      "source": [
        "* **This kernel is based on one of the exercises in the excellent book: [Deep Learning with Python by Francois Chollet](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)\n",
        "**\n",
        "* The kernel imports the IMDB reviews (originally text - already transformed by Keras to integers using a dictionary)\n",
        "* Vectorizes and normalizes the data\n",
        "* Compiles a multi layers NN\n",
        "* Monitors the learning / validation curves for loss and accuracy\n",
        "* Try and error with different layers and hidden units\n",
        "* Employs L1 and L2 weight regularization\n",
        "* Implements a DROPOUT layer\n",
        "\n",
        "* The above mentioned book is a **MUST READ**.\n",
        "* *Thanks Francois for an amazing book !*"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "MCM6IlA0Kozm"
      },
      "cell_type": "code",
      "source": [
        "# IMPORT MODULES\n",
        "# TURN ON the GPU !!!\n",
        "# If importing dataset from outside - like this IMDB - Internet must be \"connected\"\n",
        "\n",
        "import os\n",
        "from operator import itemgetter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "get_ipython().magic(u'matplotlib inline')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils, to_categorical\n",
        "\n",
        "from keras.datasets import imdb\n",
        "\n",
        "print(os.getcwd())\n",
        "print(\"Modules imported \\n\")\n",
        "print(\"Files in current directory:\")\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2309a027762ad0f83018d32eb194af33bf128021",
        "collapsed": true,
        "id": "5IN4_3mzKozp"
      },
      "cell_type": "code",
      "source": [
        "# LOAD IMDB DATA\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "563a5a8c65a26338fc42b0bcad94e3e4782be0b6",
        "id": "E6zslREnKozq"
      },
      "cell_type": "code",
      "source": [
        "print(\"train_data \", train_data.shape)\n",
        "print(\"train_labels \", train_labels.shape)\n",
        "print(\"_\"*100)\n",
        "print(\"test_data \", test_data.shape)\n",
        "print(\"test_labels \", test_labels.shape)\n",
        "print(\"_\"*100)\n",
        "print(\"Maximum value of a word index \")\n",
        "print(max([max(sequence) for sequence in train_data]))\n",
        "print(\"Maximum length num words of review in train \")\n",
        "print(max([len(sequence) for sequence in train_data]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5db6853639628b911ccdd0e0ecc484f4beeb9284",
        "id": "9-a5RdV-Kozq"
      },
      "cell_type": "code",
      "source": [
        "# See an actual review in words\n",
        "# Reverse from integers to words using the DICTIONARY (given by keras...need to do nothing to create it)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "reverse_word_index = dict(\n",
        "[(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "decoded_review = ' '.join(\n",
        "[reverse_word_index.get(i - 3, '?') for i in train_data[123]])\n",
        "\n",
        "print(decoded_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "080f1f790e7294231c1e62c1688666589d51a200",
        "collapsed": true,
        "id": "YzjDzO6mKozr"
      },
      "cell_type": "code",
      "source": [
        "# VECTORIZE as one cannot feed integers into a NN\n",
        "# Encoding the integer sequences into a binary matrix - one hot encoder basically\n",
        "# From integers representing words, at various lengths - to a normalized one hot encoded tensor (matrix) of 10k columns\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f012f8661fd781a95bd6c01197b7a93e944ff38e",
        "id": "UsXSnbjcKozr"
      },
      "cell_type": "code",
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "print(\"x_train \", x_train.shape)\n",
        "print(\"x_test \", x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d561665727e49217e14c86804f148adb6899a7b7",
        "id": "4BFprkbvKozs"
      },
      "cell_type": "code",
      "source": [
        "# VECTORIZE the labels too - NO INTEGERS only floats into a tensor...(rare exceptions)\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "print(\"y_train \", y_train.shape)\n",
        "print(\"y_test \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bccb4315c1d69c18cab22a633dc68abdff4b35fb",
        "id": "HNV7C41_Kozs"
      },
      "cell_type": "code",
      "source": [
        "# Set a VALIDATION set\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "print(\"x_val \", x_val.shape)\n",
        "print(\"partial_x_train \", partial_x_train.shape)\n",
        "print(\"y_val \", y_val.shape)\n",
        "print(\"partial_y_train \", partial_y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf2a055346b2e3252b103f1d0bab9e786e5d5eba",
        "id": "wM1gGm9DKozt"
      },
      "cell_type": "code",
      "source": [
        "# NN MODEL\n",
        "\n",
        "# Use of DROPOUT\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001),activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Use of REGULARIZATION\n",
        "#model = models.Sequential()\n",
        "#model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu', input_shape=(10000,)))\n",
        "#model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu'))\n",
        "#model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# REGULARIZERS L1 L2\n",
        "#regularizers.l1(0.001)\n",
        "#regularizers.l2(0.001)\n",
        "#regularizers.l1_l2(l1=0.001, l2=0.001)\n",
        "\n",
        "# OPTIMIZERS\n",
        "#model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])\n",
        "#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "356f3351bfa767aca48e91d305961dda2d3b9a9d",
        "scrolled": true,
        "id": "SItnn3X5Kozu"
      },
      "cell_type": "code",
      "source": [
        "# FIT / TRAIN model\n",
        "\n",
        "NumEpochs = 10\n",
        "BatchSize = 512\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(\"_\"*100)\n",
        "print(\"Test Loss and Accuracy\")\n",
        "print(\"results \", results)\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41489e2204c5d4159f5c90a350aa3ad839e03355",
        "id": "fQXBFUfBKozv"
      },
      "cell_type": "code",
      "source": [
        "# VALIDATION LOSS curves\n",
        "\n",
        "plt.clf()\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, (len(history_dict['loss']) + 1))\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abfa37123f9f403041a1c12edd6a87070b78c5ef",
        "id": "Rt6I_VPjKozv"
      },
      "cell_type": "code",
      "source": [
        "# VALIDATION ACCURACY curves\n",
        "\n",
        "plt.clf()\n",
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "epochs = range(1, (len(history_dict['acc']) + 1))\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fda90fab3c7038df8291fa4f3185fbb62e9aed4",
        "id": "qIzqOq4QKozv"
      },
      "cell_type": "code",
      "source": [
        "# PREDICT\n",
        "\n",
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}